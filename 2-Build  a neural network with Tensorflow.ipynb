{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图定义计算过程，会话执行计算过程\n",
    "# 为避免缓存变量冲突问题，每个模型都使用默认图， with tf.Graph().as_default():\n",
    "# 图定义完成之后，可以保存下来，在会话内writer = tf.summary.FileWriter(\"logs/\", sess.Graph)\n",
    "# 保存模型：saver = tf.train.Saver()，在会话内，模型训练完成后：save_path = saver.save(sess, \"my_net/demo.cpkt\")\n",
    "# 导入模型：\n",
    "# 方式一：直接导入整个图模型，不需要重复定义之前的图\n",
    "# saver = tf.train.import_meta_graph(\"my_net/demo.ckpt.meta\"), \n",
    "# 在会话内：saver.restore(sess, \"my_net/demo.ckpt\")\n",
    "# 方式二：需要重复定义之前的图\n",
    "# saver = tf.train.Saver()\n",
    "# 在会话内：saver.restore(sess, \"my_net/demo.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entropy loss\n",
    "# loss = tf.reduce_mean(-tf.reduce_sum(ys*tf.log(y_pred), reduction_indices=[1]))\n",
    "\n",
    "# dropout\n",
    "# def add_layer(xs, in_size, out_size, activation_function=None):\n",
    "#     weights = tf.Variable(tf.random_normal([in_size, out_size], dtype=tf.float32))\n",
    "#     biases = tf.Variable(tf.zeros([out_size])+0.1, dtype=tf.float32)\n",
    "#     if activation_function is None:\n",
    "#         ys = tf.nn.dropout(tf.matmul(xs, weights)+biases, keep_prob)\n",
    "#     else:\n",
    "#         ys = tf.nn.dropout(activation_function(tf.matmul(xs, weights)+biases))\n",
    "#     return ys\n",
    "\n",
    "# name scope\n",
    "# def add_layer(xs, in_size, out_size, activation_function=None):\n",
    "#     with tf.name_scope(\"layer\"):\n",
    "#         with tf.name_scope(\"weights\"):\n",
    "#             weights = tf.Variable(tf.random_normal([in_size, out_size], dtype=tf.float32))\n",
    "#         with tf.name_scope(\"biases\"):\n",
    "#             biases = tf.Variable(tf.zeros([out_size])+0.1, dtype=tf.float32)\n",
    "#         with tf.name_scope(\"ys\"):\n",
    "#             if activation_function is None:\n",
    "#                 ys = tf.matmul(xs, weights)+biases\n",
    "#             else:\n",
    "#                 ys = activation_function(tf.matmul(xs, weights)+biases)\n",
    "#             return ys     \n",
    "# with tf.name_scope('inputs'):\n",
    "#     xs = tf.placeholder(tf.float32, [None, 1], name='x_input')\n",
    "#     ys = tf.placeholder(tf.float32, [None, 1], name='y_input')\n",
    "# with tf.name_scope('loss'):\n",
    "#     loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),\n",
    "#                                         reduction_indices=[1]))\n",
    "# with tf.name_scope('train'):\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "# tensorboard\n",
    "# with tf.Session() as sess:\n",
    "#     writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "#     sess.run(init) \n",
    "# tensorboard --logdir=\"./logs\"\n",
    "\n",
    "# save the network\n",
    "# init= tf.global_variables_initializer()\n",
    "# saver = tf.train.Saver()\n",
    "# with tf.Session() as sess:\n",
    "#    sess.run(init)\n",
    "#    save_path = saver.save(sess, \"my_net/save_net.ckpt\")\n",
    "\n",
    "# restore the network\n",
    "# saver = tf.train.Saver()\n",
    "# init = tf.global_variables_initializer()\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "#     saver.restore(sess, \"my_net/save_net.cktp\")\n",
    "\n",
    "# save the network\n",
    "# W = tf.Variable([[1,2,3],[3,4,5]], dtype=tf.float32, name='weights')\n",
    "# b = tf.Variable([[1,2,3]], dtype=tf.float32, name='biases')\n",
    "# init= tf.initialize_all_variables()\n",
    "# saver = tf.train.Saver()\n",
    "# with tf.Session() as sess:\n",
    "#    sess.run(init)\n",
    "#    save_path = saver.save(sess, \"my_net/save_net.ckpt\")\n",
    "#    print(\"Save to path: \", save_path)\n",
    "\n",
    "# restore the network\n",
    "# W = tf.Variable(np.arange(6).reshape((2, 3)), dtype=tf.float32, name=\"weights\")\n",
    "# b = tf.Variable(np.arange(3).reshape((1, 3)), dtype=tf.float32, name=\"biases\")\n",
    "# # not need init step\n",
    "# saver = tf.train.Saver()\n",
    "# with tf.Session() as sess:\n",
    "#     saver.restore(sess, \"my_net/save_net.ckpt\")\n",
    "#     print(\"weights:\", sess.run(W))\n",
    "#     print(\"biases:\", sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0 0.575654\n",
      "step:  50 0.0066488246\n",
      "step:  100 0.004886212\n",
      "step:  150 0.0042625237\n",
      "step:  200 0.0039244806\n",
      "step:  250 0.0036874234\n",
      "step:  300 0.0035542662\n",
      "step:  350 0.0034145887\n",
      "step:  400 0.003331141\n",
      "step:  450 0.0032769945\n",
      "step:  500 0.0032315303\n",
      "step:  550 0.003187273\n",
      "step:  600 0.0031461902\n",
      "step:  650 0.0031141457\n",
      "step:  700 0.0030870815\n",
      "step:  750 0.0030574347\n",
      "step:  800 0.0030265069\n",
      "step:  850 0.0029934675\n",
      "step:  900 0.00296094\n",
      "step:  950 0.0029298943\n",
      "step:  1000 0.0028999264\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "\n",
    "    x = np.float32(np.linspace(-1.0, 1.0, 300)[:, np.newaxis])\n",
    "    noise = np.random.normal(0, 0.05, x.shape)\n",
    "    y = np.square(x)-0.5+noise\n",
    "    \n",
    "    with tf.name_scope(\"placeholder\"):\n",
    "        xs = tf.placeholder(shape=[300, 1], dtype=tf.float32, name=\"xs\")\n",
    "        ys = tf.placeholder(shape=[300, 1], dtype=tf.float32, name=\"ys\")\n",
    "    with tf.name_scope(\"l1\"):\n",
    "        w1 = tf.Variable(tf.random_normal(shape=[1, 10], dtype=tf.float32), name=\"w1\")\n",
    "        b1 = tf.Variable(tf.zeros(shape=[10])+0.1, dtype=tf.float32, name=\"b1\")\n",
    "        l1 = tf.nn.relu(tf.matmul(xs, w1)+b1)\n",
    "    with tf.name_scope(\"l2\"):\n",
    "        w2 = tf.Variable(tf.random_normal(shape=[10, 1]), dtype=tf.float32, name=\"w2\")\n",
    "        b2 = tf.Variable(tf.zeros(shape=[1])+0.1, dtype=tf.float32, name=\"b2\")\n",
    "        y_pred = tf.matmul(l1, w2)+b2\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-y_pred), reduction_indices=[1]))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "    train = optimizer.minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "        for step in range(1001):\n",
    "            sess.run(train, feed_dict={xs:x, ys:y})\n",
    "            if step%50 == 0:\n",
    "                print(\"step: \", step, sess.run(loss, feed_dict={xs:x, ys:y}))\n",
    "        saver.save(sess, \"my_net/demo.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from my_net/demo.ckpt\n",
      "w1:  [[ 0.53038085 -1.0457807   0.12552173  0.91430354  0.6683802  -1.4113299\n",
      "   0.6124318  -0.29901764  0.02164143  0.594167  ]]\n",
      "b1:  [ 0.08576133 -1.478195    0.06166786 -0.28530255  0.10620202 -0.6543978\n",
      "  0.19150573 -0.32496488  0.1790835   0.09040033]\n",
      "w2:  [[ 2.3643507e-01]\n",
      " [ 2.0161088e+00]\n",
      " [-1.5492942e+00]\n",
      " [ 1.4774841e+00]\n",
      " [-1.8678739e-03]\n",
      " [ 1.0833520e+00]\n",
      " [-2.9598641e-01]\n",
      " [ 4.8799190e-01]\n",
      " [-4.9223500e-01]\n",
      " [ 3.5328797e-01]]\n",
      "b2:  [-0.2777171]\n"
     ]
    }
   ],
   "source": [
    "# 只载入参数，无需重复定义图\n",
    "with tf.Graph().as_default():\n",
    "    saver = tf.train.import_meta_graph(\"my_net/demo.ckpt.meta\")\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, \"my_net/demo.ckpt\")\n",
    "        print(\"w1: \", sess.run(tf.get_default_graph().get_tensor_by_name(\"l1/w1:0\")))\n",
    "        print(\"b1: \", sess.run(tf.get_default_graph().get_tensor_by_name(\"l1/b1:0\")))\n",
    "        print(\"w2: \", sess.run(tf.get_default_graph().get_tensor_by_name(\"l2/w2:0\")))\n",
    "        print(\"b2: \", sess.run(tf.get_default_graph().get_tensor_by_name(\"l2/b2:0\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from my_net/demo.ckpt\n",
      "w1:  [[ 0.53038085 -1.0457807   0.12552173  0.91430354  0.6683802  -1.4113299\n",
      "   0.6124318  -0.29901764  0.02164143  0.594167  ]]\n",
      "b1:  [ 0.08576133 -1.478195    0.06166786 -0.28530255  0.10620202 -0.6543978\n",
      "  0.19150573 -0.32496488  0.1790835   0.09040033]\n",
      "w2:  [[ 2.3643507e-01]\n",
      " [ 2.0161088e+00]\n",
      " [-1.5492942e+00]\n",
      " [ 1.4774841e+00]\n",
      " [-1.8678739e-03]\n",
      " [ 1.0833520e+00]\n",
      " [-2.9598641e-01]\n",
      " [ 4.8799190e-01]\n",
      " [-4.9223500e-01]\n",
      " [ 3.5328797e-01]]\n",
      "b2:  [-0.2777171]\n",
      "loss:  0.0034549555\n"
     ]
    }
   ],
   "source": [
    "# 载入整个图，需要重复定义图\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    x = np.float32(np.linspace(-1.0, 1.0, 300)[:, np.newaxis])\n",
    "    noise = np.random.normal(0, 0.05, x.shape)\n",
    "    y = np.square(x)-0.5+noise\n",
    "    \n",
    "    with tf.name_scope(\"placeholder\"):\n",
    "        xs = tf.placeholder(shape=[300, 1], dtype=tf.float32, name=\"xs\")\n",
    "        ys = tf.placeholder(shape=[300, 1], dtype=tf.float32, name=\"ys\")\n",
    "    with tf.name_scope(\"l1\"):\n",
    "        w1 = tf.Variable(tf.random_normal(shape=[1, 10], dtype=tf.float32), name=\"w1\")\n",
    "        b1 = tf.Variable(tf.zeros(shape=[10])+0.1, dtype=tf.float32, name=\"b1\")\n",
    "        l1 = tf.nn.relu(tf.matmul(xs, w1)+b1)\n",
    "    with tf.name_scope(\"l2\"):\n",
    "        w2 = tf.Variable(tf.random_normal(shape=[10, 1]), dtype=tf.float32, name=\"w2\")\n",
    "        b2 = tf.Variable(tf.zeros(shape=[1])+0.1, dtype=tf.float32, name=\"b2\")\n",
    "        y_pred = tf.matmul(l1, w2)+b2\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-y_pred), reduction_indices=[1]))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "    train = optimizer.minimize(loss)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, \"my_net/demo.ckpt\")\n",
    "        print(\"w1: \", sess.run(tf.get_default_graph().get_tensor_by_name(\"l1/w1:0\")))\n",
    "        print(\"b1: \", sess.run(tf.get_default_graph().get_tensor_by_name(\"l1/b1:0\")))\n",
    "        print(\"w2: \", sess.run(tf.get_default_graph().get_tensor_by_name(\"l2/w2:0\")))\n",
    "        print(\"b2: \", sess.run(tf.get_default_graph().get_tensor_by_name(\"l2/b2:0\")))\n",
    "        print(\"loss: \", sess.run(loss, feed_dict={xs:x, ys:y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
